{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a270793f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from testsavant.guard import InputGuard\n",
    "from testsavant.guard.input_scanners import Gibberish\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b9d4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n"
     ]
    }
   ],
   "source": [
    "long_article = \"Once upon a time, there was a very long article that needed to be processed by the InputGuard system. This article contained various pieces of information, some of which were sensitive and needed to be anonymized before being sent to the language model. The InputGuard system was designed to scan the input for any sensitive data and either redact it or replace it with fake data, depending on the configuration. This ensured that the privacy of individuals was maintained while still allowing the language model to perform its tasks effectively.\"\n",
    "print(len(long_article))\n",
    "\n",
    "ts_api = InputGuard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1acfead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gibberish = Gibberish(tag=\"base\", threshold=0.0)\n",
    "ts_api.add_scanner(gibberish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f291f5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt is invalid\n",
      "Prompt is valid\n",
      "Prompt is valid\n"
     ]
    }
   ],
   "source": [
    "toxic_prompt = \"you fucking idiot, you are so stupid dhfbchbecf qekjbckjbc ihg87f324b 2ifniuc  bv2tsetr\"\n",
    "prompts = [toxic_prompt, toxic_prompt + \" \" + long_article, long_article + \" \" + toxic_prompt]\n",
    "\n",
    "for prompt in prompts:\n",
    "    result = ts_api.scan(prompt, is_async=False)\n",
    "    if result.is_valid:\n",
    "        print(\"Prompt is valid\")\n",
    "    else:\n",
    "        print(\"Prompt is invalid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b74c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
